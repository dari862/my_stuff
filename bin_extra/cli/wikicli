#!/bin/python3

import argparse
import os
import random
import urllib.request
import re
from html.parser import HTMLParser
from urllib.parse import quote
from urllib import request, error
    
# Terminal size
try:
    width, height = os.get_terminal_size()
    p = True
except OSError:
    width = 120
    height = 80
    p = False

class color:
    PURPLE = "\033[95m"
    CYAN = "\033[96m"
    DARKCYAN = "\033[36m"
    BLUE = "\033[94m"
    GREEN = "\033[92m"
    YELLOW = "\033[93m"
    RED = "\033[91m"
    BOLD = "\033[1m"
    UNDERLINE = "\033[4m"
    END = "\033[0m"

colors = [color.GREEN, color.PURPLE, color.CYAN, color.BLUE, color.DARKCYAN]
wikiurl = ""

def fetch_html(url):
    req = urllib.request.Request(url, headers={"User-Agent": "Mozilla/5.0"})
    with urllib.request.urlopen(req, timeout=15) as response:
        return response.read().decode("utf-8")
        
def req(term, lang="en"):
    global wikiurl
    encoded_term = quote(term)
    wikiurl = f"https://{lang}.wikipedia.org/wiki/{encoded_term}"
    return fetch_html(wikiurl)

class SummaryParser(HTMLParser):
    def __init__(self, limit=3):
        super().__init__()
        self.in_p = False
        self.skip_sup = False
        self.paragraphs = []
        self.buffer = []
        self.limit = limit

    def handle_starttag(self, tag, attrs):
        if tag == "p":
            self.in_p = True
            self.buffer = []
        elif tag == "sup":
            self.skip_sup = True

    def handle_endtag(self, tag):
        if tag == "p" and self.in_p:
            paragraph = "".join(self.buffer).strip()
            if paragraph:
                self.paragraphs.append(paragraph)
            self.in_p = False
        elif tag == "sup":
            self.skip_sup = False

    def handle_data(self, data):
        if self.in_p and not self.skip_sup and len(self.paragraphs) < self.limit:
            self.buffer.append(data)

class InfoParser(HTMLParser):
    def __init__(self):
        super().__init__()
        self.in_p = False
        self.in_headline = False
        self.skip_sup = False
        self.entries = []
        self.buffer = []

    def handle_starttag(self, tag, attrs):
        if tag == "p":
            self.in_p = True
            self.buffer = []
        elif tag == "sup":
            self.skip_sup = True
        elif tag == "span":
            for attr, val in attrs:
                if attr == "class" and "mw-headline" in val:
                    self.in_headline = True
                    self.buffer = []

    def handle_endtag(self, tag):
        if tag == "p" and self.in_p:
            data = "".join(self.buffer).strip()
            if data:
                self.entries.append(data)
            self.in_p = False
        elif tag == "sup":
            self.skip_sup = False
        elif tag == "span" and self.in_headline:
            data = "".join(self.buffer).strip()
            if data:
                self.entries.append("!" + data)
            self.in_headline = False

    def handle_data(self, data):
        if (self.in_p or self.in_headline) and not self.skip_sup:
            self.buffer.append(data)

def extract_summary(html):
    parser = SummaryParser()
    parser.feed(html)
    return parser.paragraphs

def extract_info(html):
    parser = InfoParser()
    parser.feed(html)
    return parser.entries

def extract_title(html):
    match = re.search(r"<title>(.*?) - Wikipedia", html)
    return match.group(1).strip() if match else "Random Article"

def extract_links(html):
    return re.findall(r'<a[^>]+title="([^"]+)"[^>]*data-serp-pos[^>]*>', html)

def extract_dym(html):
    match = re.search(r"<em>(.*?)</em>", html)
    return match.group(1) if match else None

def getSummary(term, lang="en"):
    html = req(term, lang)
    paragraphs = extract_summary(html)
    print("\n" + (color.BOLD + term).center(width, "-") + color.END)
    if any("Other reasons this message may be displayed" in p for p in paragraphs):
        print("Did you mean: ")
        searchInfo(term, called=True)
        return
    print(random.choice(colors))
    print("\n\n".join(paragraphs))
    print(color.END)

def getInfo(term, lang="en"):
    html = req(term, lang)
    entries = extract_info(html)
    if entries and "may refer to:" in entries[0]:
        searchInfo(term)
        return
    if p:
        print("\n" + (color.BOLD + term).center(width, "-") + color.END)
        print(color.BLUE + wikiurl.center(width, " ") + color.END + "\n")
    else:
        print("\n" + term.center(width, "-"))
        print(wikiurl.center(width, " ") + "\n")
    for e in entries:
        if not e.strip():
            continue
        if e in ["!See also", "!Notes", "!References", "!External links", "!Further reading"]:
            continue
        if p and e.startswith("!"):
            print(color.BOLD + random.choice(colors) + e[1:] + color.END)
            print("-" * len(e[1:]))
        elif p:
            print(color.YELLOW + "[-] " + color.END + random.choice(colors) + e + color.END)
        else:
            print(e)

def getRand(lang="en"):
    html = req("Special:Random", lang)
    title = extract_title(html)
    entries = extract_info(html)
    print("\n" + (color.BOLD + title).center(width, "-") + color.END + "\n")
    for e in entries:
        if not e.strip():
            continue
        if e in ["!See also", "!Notes", "!References", "!External links", "!Further reading"]:
            continue
        if p and e.startswith("!"):
            print(color.BOLD + random.choice(colors) + e[1:] + color.END)
            print("-" * len(e[1:]))
        elif p:
            print(color.YELLOW + "[-] " + color.END + random.choice(colors) + e + color.END)
        else:
            print(e)

def searchInfo(term, lang="en", called=False):
    encoded_term = quote(term)
    url = f"https://{lang}.wikipedia.org/w/index.php?fulltext=Search&search={encoded_term}"
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        # Create and open the request
        req_obj = request.Request(url, headers=headers)
        with request.urlopen(req_obj, timeout=15) as resp:
            final_url = resp.geturl()
            html = resp.read().decode("utf-8")

        # If Wikipedia redirected directly to an article
        if "/wiki/" in final_url and "w/index.php" not in final_url:
            getInfo(term)
            return

    except error.URLError as e:
        print("Error:", e)
        return

    # Continue normal search results handling
    links = extract_links(html)
    dym = extract_dym(html)

    if not called:
        print("Result:\n")
    if dym:
        print("Did you mean:", dym)
    for title in links:
        print(title)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("-s", "--search", help="Search any topic")
    parser.add_argument("-i", "--info", help="Get info on any topic")
    parser.add_argument("-q", "--quick", help="Get the summary on any topic")
    parser.add_argument("-l", "--lang", help="Get info in your native language (default English)")
    parser.add_argument("-x", "--rand", help="Get random Wikipedia article", action="store_true")

    a = parser.parse_args()
    if not a.lang:
        a.lang = "en"

    if a.quick:
        getSummary(a.quick, a.lang)
    if a.info:
        getInfo(a.info, a.lang)
    if a.search:
        searchInfo(a.search, a.lang)
    if a.rand:
        getRand(a.lang)

    if not (a.quick or a.info or a.search or a.rand):
        parser.print_help()
